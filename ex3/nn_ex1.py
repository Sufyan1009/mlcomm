# -*- coding: utf-8 -*-
import sys
sys.path.append('../')
import numpy as np
import matplotlib.pyplot as plt
import nn.nnet as nnn
from dataset1_nn import DataSet  

#get and plot the data
y_D,x_D = DataSet.get_data()
DataSet.plot_data()
plt.show()


#init params
nn_layers = (1,6,6,1)
nn = nnn.NNet(nn_layers, 'sigmoid', 'identity')


#Computes cost using the output generated by NN and assumed cost function
def cost():
    a = nn.output(x_D)
    cost = np.sum((y_D-a)**2)/2 
    return cost


#plot and show
DataSet.plot_model(nn.output)
plt.show()
print('cost %.3f' % cost())


#Function which contains the derivative of the cost function wrt the netowrk
#output. It is essential for the first step in the back-propgation
def dCda_f(y, a):
    return (a - y)

#gradient descent
nn.gd_learn(25000, 0.03, y_D, x_D, dCda_f)


#plot and show
DataSet.plot_model(nn.output)
plt.show()
print('cost %.3f' % cost())