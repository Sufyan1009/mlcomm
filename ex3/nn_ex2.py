# -*- coding: utf-8 -*-
import sys
sys.path.append('../')
import numpy as np
import matplotlib.pyplot as plt
import nn.nnet as nnn
from dataset2_nn import DataSet  


#get and plot the data
y_D,x_D = DataSet.get_data()
DataSet.plot_data()

lbd=0.044

#init params
nn_layers = (2,6,6,1)
nn = nnn.NNet(nn_layers, 'sigmoid', 'sigmoid')


#Computes cost using the output generated by NN and assumed cost function
def cost():
    ### YOUR CODE HERE ###
    a = nn.output(x_D)
    y_eq_0 = (y_D == 0).nonzero()[1]
    y_eq_1 = (y_D == 1).nonzero()[1]
    costa = np.sum(-np.log(a[0,y_eq_1])) + np.sum(-np.log(1 - a[0,y_eq_0]))
    for l in range(1, nn.L + 1):
        costa += lbd * np.trace(np.matmul(nn.W[l], nn.W[l].T))
    #####################
    return costa


#plot and show
DataSet.plot_decision_boundary(nn.output)
plt.show()
print('cost %.3f' % cost())



#Function which contains the derivative of the cost function wrt the netowrk
#output. It is essential for the first step in the back-propgation
def dCda_f(y,a):


    dC_da = np.nan_to_num (a-y/a*(1-a))

    ### YOUR CODE HERE ###
    
    #####################
    return dC_da


#gradient descent
nn.gd_learn(8000, 0.05, y_D, x_D, dCda_f,lbd)


#plot and show
DataSet.plot_decision_boundary(nn.output)
plt.show()
print('cost %.3f' % cost())